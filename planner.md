# LLM API Service ğŸš€

Production-style **LLM API backend service** built with **FastAPI + OpenAI Responses API**, including:

âœ… prompt template control  
âœ… strict request validation  
âœ… structured responses  
âœ… token usage tracking  
âœ… in-memory caching  
âœ… status & health endpoints  
âœ… Streamlit UI client  

> This project is **NOT RAG** and **NOT agentic AI**.  
> It focuses on **backend orchestration + production controls**.

---

## âœ¨ Features

### âœ… Backend API (FastAPI)
- Prompt template registry (`template_id`)
- Pydantic validation for request/response
- OpenAI Responses API integration
- Token usage logging: `input_tokens`, `output_tokens`, `total_tokens`
- In-memory cache with stable SHA256 hashing
- Structured error handling
- Swagger UI documentation

### âœ… UI (Streamlit)
- Calls FastAPI backend
- Shows:
  - cached status
  - token usage
  - output
  - prompt debug
- History panel
- Export history JSON
- Download output TXT

---

## ğŸ§± Tech Stack

**Backend**
- Python 3.10+
- FastAPI
- Uvicorn
- OpenAI Python SDK
- Pydantic / pydantic-settings

**UI**
- Streamlit
- Requests

---

## ğŸ“‚ Project Structure

```bash
llm-api-service/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ routes_generate.py
â”‚   â”‚       â”œâ”€â”€ routes_health.py
â”‚   â”‚       â””â”€â”€ routes_status.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ generate.py
â”‚   â”‚   â””â”€â”€ errors.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ cache.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â””â”€â”€ prompt_templates.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ hashing.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ planner.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
âš™ï¸ Setup
1) Create & activate venv
bash
Copy code
python -m venv .venv
# Windows PowerShell:
.\.venv\Scripts\Activate.ps1
2) Install dependencies
bash
Copy code
pip install -r requirements.txt
3) Configure environment variables
Option A: .env file (recommended)

Create .env in root:

env
Copy code
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini
APP_NAME=LLM API Service
APP_ENV=dev
APP_LOG_LEVEL=INFO
Option B: PowerShell env (temporary)

powershell
Copy code
$env:OPENAI_API_KEY="sk-xxxxx"
$env:OPENAI_MODEL="gpt-4o-mini"
â–¶ï¸ Run Backend API
bash
Copy code
python -m uvicorn app.main:app --reload
Swagger UI:

http://127.0.0.1:8000/docs

â–¶ï¸ Run Streamlit UI
Run backend first, then:

bash
Copy code
streamlit run ui/streamlit_app.py
UI:

http://localhost:8501

ğŸ”Œ API Endpoints
âœ… Health
http
Copy code
GET /api/v1/health
âœ… Status (templates + cache size)
http
Copy code
GET /api/v1/status
âœ… Generate Info
http
Copy code
GET /api/v1/generate
âœ… Generate (LLM call)
http
Copy code
POST /api/v1/generate
Content-Type: application/json
Example:

json
Copy code
{
  "template_id": "basic_chat_v1",
  "input": "Give 3 points why FastAPI is popular",
  "parameters": {
    "tone": "simple"
  }
}
ğŸ§  Templates
Supported templates:

basic_chat_v1

summarize_v1

Templates are stored in:
app/services/prompt_templates.py

ğŸ“Œ Interview Pitch
â€œI built a production-style LLM API service with prompt template management, validation, error handling, caching, OpenAI integration, and token usage tracking, along with a Streamlit UI client.â€

ğŸ‘¨â€ğŸ’» Author
Satya Srinath
GitHub: @satya66123
Email: satyasrinath653512@gmail.com

ğŸ“œ License
MIT LicenseLLM API SERVICE â€“ PROJECT PLANNER (COMPLETED)
===========================================

Project Name
------------
llm-api-service

Goal
----
Build a production-style backend LLM API service (no RAG, no agents) with:
- Prompt template control
- Request validation
- Error handling
- Token usage logging
- In-memory caching
- Swagger API docs
- Optional Streamlit UI client inside same repo

Tech Stack
----------
Backend:
- Python 3.10+
- FastAPI
- OpenAI API (Responses API)
- Pydantic
- Uvicorn

Caching:
- In-memory dict cache with stable SHA256 hash key

UI:
- Streamlit
- Requests

----------------------------------------------------------------

TASKS (DONE)
------------

TASK 1 â€“ Environment Setup
- Create virtual environment
- Install FastAPI, OpenAI SDK, Uvicorn
- Verify imports

TASK 2 â€“ Run FastAPI
- Created base FastAPI app
- Started server successfully using: python -m uvicorn ...

TASK 3 â€“ Production Folder Structure
- Introduced api/v1 router structure
- Health endpoint created

TASK 4 â€“ Schemas & Validation
- Added Pydantic schemas for request and response
- Swagger validations working

TASK 5 â€“ Prompt Template Engine
- Created prompt template registry
- Template validation + safe rendering

TASK 6 â€“ OpenAI Integration
- Integrated OpenAI Responses API
- Captured token usage (input/output/total)
- Returned real LLM output

TASK 7 â€“ In-memory Cache
- Stable SHA256 cache key
- cached=true/false returned
- Reduced cost + latency

TASK 8 â€“ Status Endpoint
- /api/v1/status returns templates + cache size + model + env

TASK 9 â€“ Standard Error Handling
- Global exception fallback handler
- Standard error response formatting

TASK 10 â€“ Streamlit UI (Same Repo)
- UI client to call backend
- Shows response + cached + token usage
- History panel + export JSON + download TXT

----------------------------------------------------------------

FINAL DELIVERABLES
------------------
Backend API:
- GET  /api/v1/health
- GET  /api/v1/status
- GET  /api/v1/generate
- POST /api/v1/generate

UI:
- Streamlit UI: ui/streamlit_app.py

Outcome Statement (Interview Ready)
-----------------------------------
â€œI built a production-style LLM API service with prompt template management,
request validation, caching, structured errors, token tracking, and a UI client.â€
