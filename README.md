# LLM API Service ğŸš€

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![FastAPI](https://img.shields.io/badge/FastAPI-Backend-green)
![Streamlit](https://img.shields.io/badge/Streamlit-UI-red)
![OpenAI](https://img.shields.io/badge/OpenAI-Responses%20API-black)
![License: MIT](https://img.shields.io/badge/License-MIT-yellow)
![Status](https://img.shields.io/badge/Status-Completed-brightgreen)

Production-style **LLM API backend service** built using **FastAPI + OpenAI Responses API**, including:

âœ… prompt template control  
âœ… strict request validation  
âœ… structured responses  
âœ… token usage tracking  
âœ… in-memory caching  
âœ… status & health endpoints  
âœ… Streamlit UI client  

> âš ï¸ This project is **NOT RAG** and **NOT agentic AI**.  
> It focuses purely on **backend LLM orchestration + production controls**.

---

## âœ¨ Features

### âœ… Backend API (FastAPI)
- Prompt template registry (`template_id`)
- Pydantic validation for request/response
- OpenAI Responses API integration
- Token usage tracking: `input_tokens`, `output_tokens`, `total_tokens`
- In-memory cache with stable SHA256 hashing
- Structured error handling
- Swagger UI documentation

### âœ… UI (Streamlit)
- Calls FastAPI backend
- Shows:
  - cached status
  - token usage
  - output
  - prompt debug
- History panel
- Export history JSON
- Download output TXT

---

## ğŸ§± Tech Stack

**Backend**
- Python 3.10+
- FastAPI
- Uvicorn
- OpenAI Python SDK
- Pydantic / pydantic-settings

**UI**
- Streamlit
- Requests

---

## ğŸ“‚ Project Structure


llm-api-service/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ v1/
â”‚   â”‚       â”œâ”€â”€ routes_generate.py
â”‚   â”‚       â”œâ”€â”€ routes_health.py
â”‚   â”‚       â””â”€â”€ routes_status.py
â”‚   â”‚
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py
â”‚   â”‚   â””â”€â”€ logging.py
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ generate.py
â”‚   â”‚   â””â”€â”€ errors.py
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ cache.py
â”‚   â”‚   â”œâ”€â”€ llm_client.py
â”‚   â”‚   â””â”€â”€ prompt_templates.py
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ hashing.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ planner.txt
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â””â”€â”€ .gitignore
âš™ï¸ Setup
1) Create & activate venv
bash

python -m venv .venv
Windows PowerShell

powershell

.\.venv\Scripts\Activate.ps1
2) Install dependencies
bash

pip install -r requirements.txt
3) Configure environment variables
âœ… Option A: .env file (recommended)

Create .env in root:

env

OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxx
OPENAI_MODEL=gpt-4o-mini
APP_NAME=LLM API Service
APP_ENV=dev
APP_LOG_LEVEL=INFO
âœ… Option B: PowerShell env (temporary)

powershell

$env:OPENAI_API_KEY="sk-xxxxx"
$env:OPENAI_MODEL="gpt-4o-mini"
â–¶ï¸ Run Backend API
bash

python -m uvicorn app.main:app --reload
Swagger UI:

arduino

http://127.0.0.1:8000/docs
â–¶ï¸ Run Streamlit UI
Run backend first, then:

bash

streamlit run ui/streamlit_app.py
UI:

arduino

http://localhost:8501
ğŸ”Œ API Endpoints
âœ… Health
http

GET /api/v1/health
âœ… Status (templates + cache size)
http

GET /api/v1/status
âœ… Generate Info
http

GET /api/v1/generate
âœ… Generate (LLM call)
http

POST /api/v1/generate
Example:

json

{
  "template_id": "basic_chat_v1",
  "input": "Give 3 points why FastAPI is popular",
  "parameters": {
    "tone": "simple"
  }
}
ğŸ§  Prompt Templates
Supported templates:

basic_chat_v1

summarize_v1

Templates are stored in:

bash

app/services/prompt_templates.py
ğŸª™ Token Usage Tracking (Important)
This project tracks and returns token usage per request:

input_tokens â†’ number of tokens sent to the model

output_tokens â†’ number of tokens generated by the model

total_tokens â†’ input + output

Why it matters:

backend cost estimation

usage monitoring

prompt optimization

caching efficiency validation

ğŸ§ª Quick Test (curl)
bash

curl -X POST http://127.0.0.1:8000/api/v1/generate \
  -H "Content-Type: application/json" \
  -d "{\"template_id\":\"basic_chat_v1\",\"input\":\"Explain FastAPI in 3 bullet points\"}"
ğŸ–¼ï¸ Screenshots (Optional)
Add your images inside a folder like docs/ and reference them here.

Example:

md

![Swagger UI](docs/swagger.png)
![Streamlit UI](docs/ui.png)

ğŸ‘¨â€ğŸ’» Author
Satya Srinath
GitHub: @satya66123
Email: satyasrinath653512@gmail.com

ğŸ“œ License
MIT License

MIT License

Copyright (c) 2026 Satya Srinath (@satya66123,satyasrinath653512@gmail)

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

âœ… Final Commit (after changes)

git add .
git commit -m "Finalize README"
git push







